# **Transformer_from_Scratch**  
A **Decoder-Only Transformer** built from scratch using **PyTorch** and **PyTorch Lightning**, inspired by models like **GPT**. This implementation includes **Multi-Head Attention, Feed-Forward Networks, Positional Encoding, and Layer Normalization** for efficient text generation.  

---

## **ðŸš€ Features**  
âœ… **Built from scratch** â€“ No high-level libraries like `transformers`.  
âœ… **Decoder-Only Transformer** â€“ Similar to GPT-style autoregressive models.  
âœ… **Multi-Head Attention** â€“ Captures complex word relationships.  
âœ… **Masked Self-Attention** â€“ Ensures left-to-right sequence generation.  
âœ… **Pre-Norm Transformer** â€“ Improves training stability.  
âœ… **PyTorch Lightning Integration** â€“ Simplifies training and validation.  

---

## ** Notion Documentation**  
For a **detailed explanation** of the Transformer architecture and implementation, visit the Notion page:  
ðŸ”— **[Transformer_from_Scratch Documentation](https://volageek.notion.site/Transformers-1a66151552ea801384b8d48b6e4ae1e0)**  

---

P.S: README is generated by a LLM
